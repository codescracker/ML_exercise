{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_k = torch.tensor([[1, 2, 0, 0, 0, 0], \n",
    "                      [4, 5, 6, 0, 0, 0], \n",
    "                      [7, 8, 9, 10, 0, 0], \n",
    "                      [10, 11, 12, 13, 14, 0], \n",
    "                      [13, 14, 15, 0, 0, 0]])\n",
    "\n",
    "seq_q = torch.tensor([[2, 3, 4, 0, 0],\n",
    "                        [5, 6, 0, 0, 0],\n",
    "                        [8, 9, 10, 11, 0],\n",
    "                        [11, 0, 0, 0, 0],\n",
    "                        [14, 15, 16, 17, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, len_q = seq_q.size() # batch_size x len_q\n",
    "batch_size, len_k = seq_k.size() # batch_size x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(seq_q.shape)\n",
    "print(seq_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_k.data.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_k.data.eq(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_mask = seq_k.data.eq(0).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 6])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = pad_mask.expand(batch_size, len_q, len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 6])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True,  True]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask[2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False,  True],\n",
       "        [False, False, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_ahead_mask = torch.ones(len_q, len_q).triu(1) # seq_len x seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_ahead_masking = look_ahead_mask.unsqueeze(0).expand(batch_size, len_q, len_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 5])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_masking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_masking[2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_masking[2, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "a_2 = nn.Parameter(torch.ones(d_model))\n",
    "b_2 = nn.Parameter(torch.zeros(d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = 1\n",
    "src = torch.tensor([[1, 2, 3, 4, 5, 0],\n",
    "                    [4, 5, 6, 7, 8, 0],\n",
    "                    [7, 8, 9, 10, 11, 0],\n",
    "                    [10, 11, 12, 13, 14, 0],\n",
    "                    [13, 14, 15, 16, 17, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = torch.zeros(src.size(0), 1).fill_(start_symbol).type_as(src.data) # batch_size x 1\n",
    "dec_output = dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq: torch.Tensor) -> torch.Tensor:\n",
    "    batch_size, seq_len = seq.size() # batch_size x seq_len\n",
    "    look_ahead_mask = torch.ones(seq_len, seq_len).triu(1) # seq_len x seq_len\n",
    "\n",
    "    # 1 mean masked\n",
    "    return look_ahead_mask.unsqueeze(0).expand(batch_size, seq_len, seq_len) # batch_size x seq_len x seq_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_ahead_mask = create_look_ahead_mask(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_ahead_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C, = 4, 8, 2\n",
    "x = torch.randn(B,T,C) #shape (B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7581,  0.8704],\n",
       "         [ 1.9663, -0.2711],\n",
       "         [-0.3641,  1.0965],\n",
       "         [ 1.2530,  0.5127],\n",
       "         [ 0.2694,  0.5246],\n",
       "         [-0.5033, -1.1301],\n",
       "         [ 0.5210,  0.0061],\n",
       "         [ 1.3937, -0.0289]],\n",
       "\n",
       "        [[-0.8381, -0.3315],\n",
       "         [ 0.5582,  0.6497],\n",
       "         [ 0.4310,  0.6970],\n",
       "         [-0.7077,  0.1202],\n",
       "         [ 0.7559, -1.3130],\n",
       "         [ 0.2301, -0.4101],\n",
       "         [ 0.1752, -0.4074],\n",
       "         [ 1.5727, -0.9901]],\n",
       "\n",
       "        [[ 0.1028,  0.2099],\n",
       "         [ 1.6814, -0.0090],\n",
       "         [ 0.5263,  1.4878],\n",
       "         [ 0.9548,  0.4714],\n",
       "         [ 0.4682,  1.2507],\n",
       "         [ 0.6670,  0.5630],\n",
       "         [-1.7343,  1.5960],\n",
       "         [ 1.1868,  0.3576]],\n",
       "\n",
       "        [[ 0.3733, -0.2750],\n",
       "         [ 1.0779,  1.3829],\n",
       "         [ 1.0023, -0.5159],\n",
       "         [ 0.5725,  1.4167],\n",
       "         [ 0.3471, -0.2192],\n",
       "         [-0.4475,  0.3757],\n",
       "         [ 2.0864, -0.1763],\n",
       "         [ 0.6250,  0.1615]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention matrix (lower triangular), a mask used to only show previous items to predict next item\n",
    "wei = torch.tril(torch.ones((T,T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.sum(dim=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = wei/wei.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = torch.matmul(w2, x)\n",
    "out2 = w2 @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True],\n",
       "         [True, True]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 == out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d,\n",
    "        H,\n",
    "        T,\n",
    "        bias=False,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        d: size of embedding dimension\n",
    "        H: number of attention heads\n",
    "        T: maximum length of input sequences (in tokens)\n",
    "        bias: whether or not to use bias in linear layers\n",
    "        dropout: probability of dropout\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d % H == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        # output is 3X the dimension because it includes key, query and value\n",
    "        self.c_attn = nn.Linear(d, 3*d, bias=bias)\n",
    "\n",
    "        # projection of concatenated attention head outputs\n",
    "        self.c_proj = nn.Linear(d, d, bias=bias)\n",
    "\n",
    "        # dropout modules\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.H = H\n",
    "        self.d = d\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to\n",
    "        # the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(T, T))\n",
    "                                    .view(1, 1, T, T))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.size() # batch size, sequence length, embedding dimensionality\n",
    "\n",
    "        # compute query, key, and value vectors for all heads in batch\n",
    "        # split the output into separate query, key, and value tensors\n",
    "        q, k, v  = self.c_attn(x).split(self.d, dim=2) # [B, T, d]\n",
    "\n",
    "        # reshape tensor into sequences of smaller token vectors for each head\n",
    "        k = k.view(B, T, self.H, self.d // self.H).transpose(1, 2) # [B, H, T, d // H]\n",
    "        q = q.view(B, T, self.H, self.d // self.H).transpose(1, 2)\n",
    "        v = v.view(B, T, self.H, self.d // self.H).transpose(1, 2)\n",
    "\n",
    "        # compute the attention matrix, perform masking, and apply dropout\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # [B, H, T, T]\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        # compute output vectors for each token\n",
    "        y = att @ v # [B, H, T, d // H]\n",
    "\n",
    "        # concatenate outputs from each attention head and linearly project\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, self.d)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 512\n",
    "H = 8\n",
    "T = 1024\n",
    "bias=True\n",
    "dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CausalSelfAttention(d, H, T, bias=bias, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mask[:,:,:T,:T].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 32\n",
    "t = 512\n",
    "n_tok = 8\n",
    "dim = 1024\n",
    "\n",
    "probs = torch.randn(b*t, n_tok) # shape (b*t, n_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_idx = 4\n",
    "expert_idx = 2\n",
    "\n",
    "p = probs[tok_idx, expert_idx, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(b*t, dim) # shape (b*t, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[tok_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[tok_idx]*p).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
